#!/bin/bash
#
# Submit jobs to I-TASSER server
#
#
# [args]: 1- directory with ONLY 2 files: multi fasta file and parameters.itasser with user account information
#		  2- dir where results will be saved
#
#
# docker run --rm -v /workingdir:/data pegi3s/i-tasser_server bash -c "/opt/run /data/INPUT /data/OUTPUT"
#
#------------------------------------------------------------------------------

# Set input and output folder names
if [[ ! -z "$1" ]] && [[ ! -z "$2" ]];
then
	INPUTDIR=$1
	OUTDIR=$2
else
	INPUTDIR=data/input
	OUTDIR=data/Results_I-TASSER
fi

# Variable init
ACCOUNT=$(cat $INPUTDIR/parameters.itasser)
SLEEP=$(echo $(( $RANDOM % 5 + 3 ))) 
FILENAME=$(ls $INPUTDIR | grep -v 'parameters.itasser')

# Remove line breaks
docker run --rm -v $(pwd):/data pegi3s/utilities fasta_remove_line_breaks /data/$INPUTDIR/$FILENAME

# Parse and submit each sequence (1 at a time); Creates copy file with "<" between header and seq (because of while...read syntax)
sed '1~2 s/$/</' $INPUTDIR/$FILENAME | sed ':a;N;$!ba;s/<\n/</g' > $INPUTDIR/$FILENAME.copy

while IFS=$'<' read -r header seq;
do
	resDir=$(echo "$header" | sed 's/[^A-Za-z0-9]/_/g') && mkdir -p $OUTDIR/$resDir && touch $OUTDIR/$resDir/fileJob.fasta
	echo "$header" > $OUTDIR/$resDir/fileJob.fasta
	echo "$seq" >> $OUTDIR/$resDir/fileJob.fasta
	job="$OUTDIR/$resDir/fileJob.fasta"

	# Job submission
	mkdir -p $OUTDIR/$resDir/tempFiles
	curl -F seq_file="@$job" $ACCOUNT -F TARGET-NAME="$header" -F PRI="no" https://zhanglab.ccmb.med.umich.edu/cgi-bin/itasser_submit.cgi > $OUTDIR/$resDir/tempFiles/submission.html

	# Getting the results URL
	perl -nle 'print $1 if /<a.*=(.+?)>/' $OUTDIR/$resDir/tempFiles/submission.html | head -1 > $OUTDIR/$resDir/tempFiles/link.tmp
	res=$(cat $OUTDIR/$resDir/tempFiles/link.tmp)

	# Getting the source code
	wget -O - $res > $OUTDIR/$resDir/tempFiles/results.html

	# Dealing with failed submissions ($OUTDIR/$resDir_failed)
	lynx --dump $OUTDIR/$resDir/tempFiles/submission.html > $OUTDIR/$resDir/tempFiles/submission.txt
	
	# If < 10aa
	grep -q -s 'short' $OUTDIR/$resDir/tempFiles/submission.txt
	error_status_short=$?

	# If > 1500aa
	grep -q -s 'long' $OUTDIR/$resDir/tempFiles/submission.txt
	error_status_long=$?

	if (( ! $error_status_short ))
	then
		printf -- "$(head -3 $OUTDIR/$resDir/tempFiles/submission.txt)\n"

		# Move html to failed directory & remove temporary files
		mkdir -p $OUTDIR/${resDir}_failed
		cp $OUTDIR/$resDir/tempFiles/submission.html $OUTDIR/${resDir}_failed && rm -rf $OUTDIR/$resDir
	
	elif (( ! $error_status_long ))
	then
		printf -- "$(head -5 $OUTDIR/$resDir/tempFiles/submission.txt)\n"

		# Move html to failed directory & remove temporary files
		mkdir -p $OUTDIR/${resDir}_failed
		cp $OUTDIR/$resDir/tempFiles/submission.html $OUTDIR/${resDir}_failed && rm -rf $OUTDIR/$resDir
	fi

	# Checking if results are ready
	if [[ "$error_status_short" != "0" && "$error_status_long" != "0" ]]
	then
		grep -q "I-TASSER results for" $OUTDIR/$resDir/tempFiles/results.html ; ready=$? # grep exit status 1 if lines not matched, 0 if matched

		while [[ $ready -eq 1 ]]; 
		do #ready == 1, means results are not ready
			sleep ${SLEEP}h
			# Visit url again
			wget -O - $res > $OUTDIR/$resDir/tempFiles/results.html
			grep -q "I-TASSER results for" $OUTDIR/$resDir/tempFiles/results.html ; ready=$?
		done
		
		# Move results link
		mv $OUTDIR/$resDir/tempFiles/results.html $OUTDIR/$resDir

		# Get results from URL (all 5 models)
		for (( i = 1; i <=5 ; i++ )); do
			wget -O - $(echo ${res}model${i}.pdb) 2>/dev/null > $OUTDIR/$resDir/model${i}.pdb
		done

		# Remove models that dont exist (because wget still creates the files)
		find $OUTDIR/$resDir -empty -type f -delete

		# Remove temp folder
		rm -rf $OUTDIR/$resDir/tempFiles
	fi

done < $INPUTDIR/$FILENAME.copy

# Remove copy file
rm -f $INPUTDIR/$FILENAME.copy